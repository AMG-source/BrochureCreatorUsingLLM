{
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There might be a problem with your API key? Please visit the troubleshooting notebook!\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "# Initialize and constants\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "MODEL = 'llama3.2'\n",
    "openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a tutor, answer the questions and explain in easy words\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break down this line of code.\n",
       "\n",
       "This is written in Python and utilizes a feature called \"generators\" or more specifically, the `yield from` statement.\n",
       "\n",
       "Here's what it does:\n",
       "\n",
       "1. It iterates over an iterable (like a list or another generator) named `books`.\n",
       "2. For each item (`book`) in the iterable:\n",
       "   - It calls the method `.get(\"author\")` on that `book`, which presumably returns the author's name.\n",
       "3. If there is such a key-value pair in the `book' dictionary, it yields the value associated with `\"author\"`.\n",
       "\n",
       "So in essence, this line generates all authors from multiple books, without having to load anything into memory at once. In other words, you get one author per yield instruction call.\n",
       "\n",
       "The reason for using 'yield from'? Without `yield from`, generators would need to collect and return every item on their own (without using some form of recursion or higher-order function). But by prefixing it with `yield from`, Python allows a generator to delegate to its child iterable, instead. The parent can then generate one value at a time, but doesn't have any control over the values.\n",
       "\n",
       "Think of it like passing along responsibility:\n",
       "\n",
       "     A   |   B\n",
       "  ---  |\n",
       "    +---+    \n",
       "       |   yield from books\n",
       "    \n",
       "    In this example:\n",
       "   - `A` is the generator function.\n",
       "   - While `B` generates 'i' in all its recursive calls, this way avoids the huge amount of memory consumed by recursion for some big inputs!\n",
       "- Let's look at an example with code.\n",
       "\n",
       "```python\n",
       "def get_authors(books):\n",
       "    \"\"\"Get authors from a list of books\"\"\"\n",
       "    \n",
       "    for book in books:\n",
       "        yield from {book.get(\"author\")}    \n",
       "\n",
       "# Then call it like this\n",
       "\n",
       "books = [\n",
       "  {'title': 'The title of the first book.' , 'publisher' : ' Publisher one', 'author' : \"Author ONE\"},\n",
       "  {'title': 'The title of the second book.' , 'publisher' : 'Publisher Two' , 'author' : \"Author TWO\"},\n",
       "]\n",
       "\n",
       "for author in get_authors(books):\n",
       "    print(author)\n",
       "```\n",
       "When we run this, it prints: Author ONE and then Author TWO."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    \n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "\n",
    "\n",
    "display(Markdown(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c42043-d939-4ca7-b90d-4bf0966ab045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
